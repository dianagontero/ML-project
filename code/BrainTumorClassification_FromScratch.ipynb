{"cells":[{"cell_type":"markdown","metadata":{"id":"kCzsNt71zkzn"},"source":["# Imports & Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjVWIF5Jq0Ea"},"outputs":[],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/ProjectML/data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvQdpHrGq6Df"},"outputs":[],"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torchvision import models, transforms\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"U3uVghnFzpby"},"source":["# Data Loading & Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Cm-VtWazvaJ"},"outputs":[],"source":["TRAIN_PATH = \"training.npz\"\n","TEST_PATH  = \"testing.npz\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AwflCUxiq8FM"},"outputs":[],"source":["train_npz = np.load(TRAIN_PATH, allow_pickle=True)\n","test_npz  = np.load(TEST_PATH,  allow_pickle=True)\n","\n","X_full      = train_npz['images']\n","y_full      = train_npz['labels']\n","class_names = train_npz['class_names']\n","\n","X_test = test_npz['images']\n","y_test = test_npz['labels']\n","\n","print(\"Full (to split):\", X_full.shape, y_full.shape)\n","print(\"Test:\", X_test.shape, y_test.shape)\n","print(\"Class names:\", class_names)\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_full, y_full,\n","    test_size=0.2,\n","    random_state=42,\n","    stratify=y_full\n",")\n","\n","print(\"Train:\", X_train.shape, y_train.shape)\n","print(\"Val:\",   X_val.shape,   y_val.shape)\n","print(\"Test:\",  X_test.shape,  y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nM2-aWCSxBaV"},"outputs":[],"source":["from PIL import Image\n","\n","#first experiment\n","class NumpyDataset(Dataset):\n","    def __init__(self, X, y, transform=None):\n","        self.X = X\n","        self.y = y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        img = self.X[idx]\n","        label = self.y[idx]\n","\n","        if img.ndim == 3 and img.shape[-1] in [1, 3]:\n","            pass\n","        else:\n","            raise ValueError(f\"Image with shape not compatible: {img.shape}\")\n","\n","        if img.dtype != np.uint8:\n","            img = (img * 255).astype(np.uint8)\n","        img = Image.fromarray(img)\n","\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        label = torch.tensor(label).long()\n","        return img, label\n","\n","\n","\"\"\"\n","#experiment\n","class TargetedAugmentDataset(Dataset):\n","    def __init__(self, X, y, strong_classes=[0,1], transform_basic=None, transform_strong=None):\n","        self.X = X\n","        self.y = y\n","        self.strong_classes = strong_classes  # es: [0,1] = glioma, meningioma\n","        self.transform_basic = transform_basic\n","        self.transform_strong = transform_strong\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        img = self.X[idx]\n","        label = self.y[idx]\n","\n","        if img.dtype != np.uint8:\n","            img = (img * 255).astype(np.uint8)\n","\n","        img = Image.fromarray(img)\n","\n","        if label in self.strong_classes:\n","            if self.transform_strong:\n","                img = self.transform_strong(img)\n","        else:\n","            if self.transform_basic:\n","                img = self.transform_basic(img)\n","\n","        label = torch.tensor(label).long()\n","        return img, label\n","\n","#end experiment\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYKxFjo3qFe3"},"outputs":[],"source":["\n","#first experiment\n","data_transforms = {\n","    #more augmentation becauase no pretrained data\n","    'train': transforms.Compose([\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(15),\n","        transforms.RandomResizedCrop(size=(X_train.shape[1], X_train.shape[2]), scale=(0.9,1.0)),\n","        transforms.ColorJitter(brightness=0.15, contrast=0.2),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","    ]),\n","}\n","\"\"\"\n","#experiment\n","# Augmentation STANDARD (easy classes)\n","basic_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","])\n","\n","# Augmentation STRONG (glioma + meningioma)\n","strong_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(20),\n","    transforms.RandomResizedCrop(size=(X_train.shape[1], X_train.shape[2]), scale=(0.85,1.0)),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.25),\n","    transforms.RandomAffine(degrees=0, translate=(0.05,0.05)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","])\n","#end experiment\n","\"\"\"\n","batch_size = 32\n","num_workers = 2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","#first experiment\n","train_ds = NumpyDataset(X_train, y_train, transform=data_transforms['train'])\n","val_ds   = NumpyDataset(X_val,   y_val,   transform=data_transforms['val'])\n","test_ds  = NumpyDataset(X_test,  y_test,  transform=data_transforms['test'])\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n","val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n","test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n","\n","\"\"\"\n","experiment\n","train_ds = TargetedAugmentDataset(\n","    X_train, y_train,\n","    strong_classes=[0,1],\n","    transform_basic=basic_transform,\n","    transform_strong=strong_transform\n",")\n","\n","val_ds = NumpyDataset(X_val, y_val, transform=basic_transform)\n","test_ds = NumpyDataset(X_test, y_test, transform=basic_transform)\n","\"\"\"\n","train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)\n","val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2)\n","test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False, num_workers=2)\n","\n","dataset_sizes = {'train': len(train_ds), 'val': len(val_ds), 'test': len(test_ds)}\n","print(dataset_sizes)\n","\n","#experiment\n","class_weights = torch.tensor([1.0, 1.4, 1.0, 1.0], dtype=torch.float).to(device)\n","#end experiment"]},{"cell_type":"markdown","metadata":{"id":"4axnKWUyz34-"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dHyzrSMrJEF"},"outputs":[],"source":["num_classes = len(np.unique(y_train))\n","\"\"\"\n","model = models.mobilenet_v2(pretrained=True)\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","model = model.to(device)\n","\n","# Freeze\n","for param in model.features.parameters():\n","    param.requires_grad = False\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.classifier.parameters(), lr=1e-3)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n","\"\"\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dNcUbkSSllD"},"outputs":[],"source":["class SimpleCNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(SimpleCNN, self).__init__()\n","        #Kernel 3x3 standard\n","        #Padding = 1 -> preserve spatial dimension (needed for fully connected layer)\n","        #channels: 32->64->128->256\n","        #maxPool2d = 2 -> halves H and W at each layer, making the model more robust and reducing parameters\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","\n","            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2)\n","        )\n","\n","        self.pool = nn.AdaptiveAvgPool2d((1,1))\n","\n","        self.classifier = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(256*1*1, 256),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(256, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.pool(x) #reduce HxW to 1x1\n","        x = self.classifier(x)\n","        return x\n","model = SimpleCNN(num_classes=num_classes).to(device)\n","\"\"\"\n","#experiment\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=2, weight=None):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.weight = weight\n","        self.ce = nn.CrossEntropyLoss(weight=weight)\n","\n","    def forward(self, inputs, targets):\n","        logp = self.ce(inputs, targets)\n","        p = torch.exp(-logp)\n","        loss = ((1 - p) ** self.gamma * logp).mean()\n","        return loss\n","\"\"\"\n","\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n","#criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n","\n","#end experiment\n","\n","\"\"\"first experiment\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"xI8ev0aRz6ZF"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"AJwj0Ndv4yWf"},"source":["**Early stopping logic**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do2fo35j4x1B"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0.0, verbose=False):\n","        \"\"\"\n","        Args:\n","            patience (int): how many epochs to wait after last improvement\n","            min_delta (float): minimal change to qualify as an improvement\n","            verbose (bool): whether to print when early‚Äêstop is triggered\n","        \"\"\"\n","        self.patience    = patience\n","        self.min_delta   = min_delta\n","        self.verbose     = verbose\n","        self.best_loss   = float('inf')\n","        self.counter     = 0\n","        self.early_stop  = False\n","\n","    def __call__(self, val_loss, model, best_model_wts):\n","        if val_loss < self.best_loss - self.min_delta:\n","            self.best_loss  = val_loss\n","            self.counter    = 0\n","            best_model_wts  = model.state_dict().copy()\n","        else:\n","            self.counter += 1\n","            if self.verbose:\n","                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        return best_model_wts\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dr6Ph2iZrMNw"},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=5, early_stopping=None):\n","    best_model_wts = model.state_dict().copy()\n","    best_acc = 0.0\n","\n","    history = {\n","        'train_loss': [],\n","        'val_loss':   [],\n","        'train_acc':  [],\n","        'val_acc':    []\n","    }\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print('-'*20)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","                dataloader = train_loader\n","            else:\n","                model.eval()\n","                dataloader = val_loader\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            with tqdm(dataloader, unit=\"batch\", desc=f\"{phase} Epoch {epoch+1}\") as tepoch:\n","                for inputs, labels in tepoch:\n","                    inputs = inputs.to(device)\n","                    labels = labels.to(device)\n","\n","                    optimizer.zero_grad()\n","                    with torch.set_grad_enabled(phase == 'train'):\n","                        outputs = model(inputs)\n","                        _, preds = torch.max(outputs, 1)\n","                        loss = criterion(outputs, labels)\n","\n","                        if phase == 'train':\n","                            loss.backward()\n","                            optimizer.step()\n","\n","                    running_loss += loss.item() * inputs.size(0)\n","                    running_corrects += torch.sum(preds == labels.data)\n","\n","                    current_loss = running_loss / ((tepoch.n * tepoch.last_print_n) if tepoch.last_print_n else 1)\n","                    tepoch.set_postfix(loss=f\"{loss.item():.4f}\")\n","\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n","\n","            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n","\n","            if phase == 'train':\n","                history['train_loss'].append(epoch_loss)\n","                history['train_acc'].append(epoch_acc.item())\n","            else:\n","                history['val_loss'].append(epoch_loss)\n","                history['val_acc'].append(epoch_acc.item())\n","\n","                if early_stopping is not None:\n","                    best_model_wts = early_stopping(epoch_loss, model, best_model_wts)\n","                    if early_stopping.early_stop:\n","                        print(\"Early stopping triggered.\")\n","                        model.load_state_dict(best_model_wts)\n","                        return model, history\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = model.state_dict().copy()\n","\n","\n","\n","    print(f\"Best val Acc: {best_acc:.4f}\")\n","    model.load_state_dict(best_model_wts)\n","    return model, history\n"]},{"cell_type":"markdown","metadata":{"id":"eVLWyZ191bb2"},"source":["**Hyperparamms**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"IBg6Sw3q1bDu"},"outputs":[],"source":["\n","es = EarlyStopping(patience=5, min_delta=0.01, verbose=True)\n","model, history = train_model(model, criterion, optimizer, scheduler, num_epochs=40, early_stopping=es)\n","\n","\"\"\"LR = 1e-5\n","STEP_SIZE = 7\n","GAMMA = 0.1\n","EPOCHS = 30\n","BATCH_SIZE = 32\n","PATIENCE = 3\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"0Ep8rEznrRrP"},"outputs":[],"source":["\"\"\"\n","for param in model.features.parameters():\n","    param.requires_grad = True\n","\"\"\"\n","\"\"\"optimizer_ft = optim.Adam(model.parameters(), lr=LR)\n","exp_lr_scheduler_ft = lr_scheduler.StepLR(optimizer_ft, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","es = EarlyStopping(patience=PATIENCE, min_delta=0.01, verbose=True)\n","\n","model, history = train_model(\n","    model,\n","    criterion,\n","    optimizer_ft,\n","    exp_lr_scheduler_ft,\n","    num_epochs=EPOCHS,\n","    early_stopping=es\n",")\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XVmX7rqZ3Fn5"},"outputs":[],"source":["epochs = range(1, len(history['train_loss']) + 1)\n","\n","plt.figure(figsize=(12,5))\n","\n","# Loss subplot\n","plt.subplot(1,2,1)\n","plt.plot(epochs, history['train_loss'], label='Train Loss')\n","plt.plot(epochs, history['val_loss'],   label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training & Validation Loss')\n","plt.legend()\n","\n","# Accuracy subplot\n","plt.subplot(1,2,2)\n","plt.plot(epochs, history['train_acc'], label='Train Acc')\n","plt.plot(epochs, history['val_acc'],   label='Val Acc')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training & Validation Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pqzsNJJH3KVb"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEjJokdvrXO1"},"outputs":[],"source":["model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","all_preds = np.array(all_preds)\n","all_labels = np.array(all_labels)\n","\n","print(\"Classification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=class_names))\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","plt.figure(figsize=(8,6))\n","sns.heatmap(cm, annot=True, fmt=\"d\",\n","            xticklabels=class_names,\n","            yticklabels=class_names,\n","            cmap=plt.cm.Blues)\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.title('Confusion Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uiHXwOu63MA-"},"source":["# Save Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kSsitDagreeq"},"outputs":[],"source":["os.makedirs(\"models\", exist_ok=True)\n","torch.save(model.state_dict(), 'models/simple_cnn.pth')"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}