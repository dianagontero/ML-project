{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Imports & Drive"],"metadata":{"id":"kCzsNt71zkzn"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd drive/MyDrive/ProjectML/data"],"metadata":{"id":"RjVWIF5Jq0Ea"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torchvision import models, transforms\n","from tqdm.auto import tqdm\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"AvQdpHrGq6Df"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Loading & Preprocessing"],"metadata":{"id":"U3uVghnFzpby"}},{"cell_type":"code","source":["TRAIN_PATH = \"training.npz\"\n","TEST_PATH  = \"testing.npz\""],"metadata":{"id":"4Cm-VtWazvaJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_npz = np.load(TRAIN_PATH, allow_pickle=True)\n","test_npz  = np.load(TEST_PATH,  allow_pickle=True)\n","\n","X_full      = train_npz['images']\n","y_full      = train_npz['labels']\n","class_names = train_npz['class_names']\n","\n","X_test = test_npz['images']\n","y_test = test_npz['labels']\n","\n","print(\"Full (to split):\", X_full.shape, y_full.shape)\n","print(\"Test:\", X_test.shape, y_test.shape)\n","print(\"Class names:\", class_names)\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_full, y_full,\n","    test_size=0.2,\n","    random_state=42,\n","    stratify=y_full\n",")\n","\n","print(\"Train:\", X_train.shape, y_train.shape)\n","print(\"Val:\",   X_val.shape,   y_val.shape)\n","print(\"Test:\",  X_test.shape,  y_test.shape)"],"metadata":{"id":"AwflCUxiq8FM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NumpyDataset(Dataset):\n","    def __init__(self, X, y, transform=None):\n","        self.X = X\n","        self.y = y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        img = self.X[idx]\n","        label = self.y[idx]\n","\n","        if img.ndim == 3 and img.shape[-1] in [1,3]:\n","            img = np.transpose(img, (2,0,1))\n","\n","        img = torch.from_numpy(img).float()\n","        if self.transform:\n","            img = self.transform(img)\n","\n","        label = torch.tensor(label).long()\n","        return img, label\n"],"metadata":{"id":"nM2-aWCSxBaV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rYKxFjo3qFe3"},"outputs":[],"source":["data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(20),\n","        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","    ]),\n","    'test': transforms.Compose([\n","        transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n","    ]),\n","}\n","\n","batch_size = 32\n","num_workers = 2\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","train_ds = NumpyDataset(X_train, y_train, transform=data_transforms['train'])\n","val_ds   = NumpyDataset(X_val,   y_val,   transform=data_transforms['val'])\n","test_ds  = NumpyDataset(X_test,  y_test,  transform=data_transforms['test'])\n","\n","train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=num_workers, pin_memory=True)\n","val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n","test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n","\n","dataset_sizes = {'train': len(train_ds), 'val': len(val_ds), 'test': len(test_ds)}\n","print(dataset_sizes)"]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"4axnKWUyz34-"}},{"cell_type":"code","source":["num_classes = len(np.unique(y_train))\n","\n","model = models.mobilenet_v2(pretrained=True)\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","model = model.to(device)\n","\n","# Freeze\n","for param in model.features.parameters():\n","    param.requires_grad = False\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.classifier.parameters(), lr=1e-3)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"],"metadata":{"id":"2dHyzrSMrJEF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"xI8ev0aRz6ZF"}},{"cell_type":"markdown","source":["**Early stopping logic**"],"metadata":{"id":"AJwj0Ndv4yWf"}},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0.0, verbose=False):\n","        \"\"\"\n","        Args:\n","            patience (int): how many epochs to wait after last improvement\n","            min_delta (float): minimal change to qualify as an improvement\n","            verbose (bool): whether to print when early‐stop is triggered\n","        \"\"\"\n","        self.patience    = patience\n","        self.min_delta   = min_delta\n","        self.verbose     = verbose\n","        self.best_loss   = float('inf')\n","        self.counter     = 0\n","        self.early_stop  = False\n","\n","    def __call__(self, val_loss, model, best_model_wts):\n","        if val_loss < self.best_loss - self.min_delta:\n","            self.best_loss  = val_loss\n","            self.counter    = 0\n","            best_model_wts  = model.state_dict().copy()\n","        else:\n","            self.counter += 1\n","            if self.verbose:\n","                print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        return best_model_wts\n"],"metadata":{"id":"Do2fo35j4x1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=5, early_stopping=None):\n","    best_model_wts = model.state_dict().copy()\n","    best_acc = 0.0\n","\n","    history = {\n","        'train_loss': [],\n","        'val_loss':   [],\n","        'train_acc':  [],\n","        'val_acc':    []\n","    }\n","\n","    for epoch in range(num_epochs):\n","        print(f\"Epoch {epoch+1}/{num_epochs}\")\n","        print('-'*20)\n","\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()\n","                dataloader = train_loader\n","            else:\n","                model.eval()\n","                dataloader = val_loader\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            with tqdm(dataloader, unit=\"batch\", desc=f\"{phase} Epoch {epoch+1}\") as tepoch:\n","                for inputs, labels in tepoch:\n","                    inputs = inputs.to(device)\n","                    labels = labels.to(device)\n","\n","                    optimizer.zero_grad()\n","                    with torch.set_grad_enabled(phase == 'train'):\n","                        outputs = model(inputs)\n","                        _, preds = torch.max(outputs, 1)\n","                        loss = criterion(outputs, labels)\n","\n","                        if phase == 'train':\n","                            loss.backward()\n","                            optimizer.step()\n","\n","                    running_loss += loss.item() * inputs.size(0)\n","                    running_corrects += torch.sum(preds == labels.data)\n","\n","                    current_loss = running_loss / ((tepoch.n * tepoch.last_print_n) if tepoch.last_print_n else 1)\n","                    tepoch.set_postfix(loss=f\"{loss.item():.4f}\")\n","\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc  = running_corrects.double() / dataset_sizes[phase]\n","\n","            print(f\"{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n","\n","            if phase == 'train':\n","                history['train_loss'].append(epoch_loss)\n","                history['train_acc'].append(epoch_acc.item())\n","            else:\n","                history['val_loss'].append(epoch_loss)\n","                history['val_acc'].append(epoch_acc.item())\n","\n","                if early_stopping is not None:\n","                    best_model_wts = early_stopping(epoch_loss, model, best_model_wts)\n","                    if early_stopping.early_stop:\n","                        print(\"Early stopping triggered.\")\n","                        model.load_state_dict(best_model_wts)\n","                        return model, history\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = model.state_dict().copy()\n","\n","\n","\n","    print(f\"Best val Acc: {best_acc:.4f}\")\n","    model.load_state_dict(best_model_wts)\n","    return model, history\n"],"metadata":{"id":"Dr6Ph2iZrMNw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Hyperparamms**"],"metadata":{"id":"eVLWyZ191bb2"}},{"cell_type":"code","source":["LR = 1e-5\n","STEP_SIZE = 7\n","GAMMA = 0.1\n","EPOCHS = 30\n","BATCH_SIZE = 32\n","PATIENCE = 3"],"metadata":{"id":"IBg6Sw3q1bDu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for param in model.features.parameters():\n","    param.requires_grad = True\n","\n","optimizer_ft = optim.Adam(model.parameters(), lr=LR)\n","exp_lr_scheduler_ft = lr_scheduler.StepLR(optimizer_ft, step_size=STEP_SIZE, gamma=GAMMA)\n","\n","es = EarlyStopping(patience=PATIENCE, min_delta=0.01, verbose=True)\n","\n","model, history = train_model(\n","    model,\n","    criterion,\n","    optimizer_ft,\n","    exp_lr_scheduler_ft,\n","    num_epochs=EPOCHS,\n","    early_stopping=es\n",")"],"metadata":{"id":"0Ep8rEznrRrP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = range(1, len(history['train_loss']) + 1)\n","\n","plt.figure(figsize=(12,5))\n","\n","# Loss subplot\n","plt.subplot(1,2,1)\n","plt.plot(epochs, history['train_loss'], label='Train Loss')\n","plt.plot(epochs, history['val_loss'],   label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training & Validation Loss')\n","plt.legend()\n","\n","# Accuracy subplot\n","plt.subplot(1,2,2)\n","plt.plot(epochs, history['train_acc'], label='Train Acc')\n","plt.plot(epochs, history['val_acc'],   label='Val Acc')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.title('Training & Validation Accuracy')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"XVmX7rqZ3Fn5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testing"],"metadata":{"id":"pqzsNJJH3KVb"}},{"cell_type":"code","source":["model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","all_preds = np.array(all_preds)\n","all_labels = np.array(all_labels)\n","\n","print(\"Classification Report:\")\n","print(classification_report(all_labels, all_preds, target_names=class_names))\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","plt.figure(figsize=(8,6))\n","sns.heatmap(cm, annot=True, fmt=\"d\",\n","            xticklabels=class_names,\n","            yticklabels=class_names,\n","            cmap=plt.cm.Blues)\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.title('Confusion Matrix')\n","plt.show()"],"metadata":{"id":"xEjJokdvrXO1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Checkpoint"],"metadata":{"id":"uiHXwOu63MA-"}},{"cell_type":"code","source":["torch.save(model.state_dict(), 'models/mobilenetv2_finetuned.pth')"],"metadata":{"id":"kSsitDagreeq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","# Heatmap generation\n"],"metadata":{"id":"AfsbP4w3027k"}},{"cell_type":"markdown","source":["Uses the trained model: The Grad-CAM code takes your trained MobileNetV2 model (which was already trained in the earlier cells)\n","Computes gradients: When you pass an image through the model, Grad-CAM:\n","\n","Does a forward pass to get predictions\n","Does a backward pass to compute gradients\n","Uses these gradients to identify which regions of the image were most important for the prediction\n","\n","\n","Generates heatmaps on-the-fly: Each time you run the Grad-CAM cells, it generates activation maps in real-time for whatever images you show it"],"metadata":{"id":"KIUfb8qd2Ja5"}},{"cell_type":"markdown","source":["## Import libraries"],"metadata":{"id":"Dok5Y3t81Dti"}},{"cell_type":"code","source":["import cv2\n","from torch.nn import functional as F\n","from matplotlib import cm"],"metadata":{"id":"CCCoFQxU06Ya"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the already trained model"],"metadata":{"id":"TXWakUHx-868"}},{"cell_type":"code","source":["MODEL_PATH = \"models/mobilenetv2_finetuned.pth\"\n","# Load the saved model\n","num_classes = len(class_names)\n","\n","# Recreate the model architecture\n","model = models.mobilenet_v2(pretrained=False)  # Don't need pretrained weights now\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n","\n","# Load the saved weights\n","model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","model = model.to(device)\n","model.eval()  # Set to evaluation mode\n","\n","print(f\"Model loaded successfully from {MODEL_PATH}\")\n","print(f\"Model is on device: {next(model.parameters()).device}\")"],"metadata":{"id":"yscUULvm--tW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Grad-CAM Implementation"],"metadata":{"id":"AtKK0clZ1CN-"}},{"cell_type":"markdown","source":["First i save the the last feature layers i want to analyze which is usually the last convulation layer.\n","\n","Define the register hook that capture the activations of the forward pass(output) and the backward pass, which captures the gradients during backpropagation. The magnitude of the gradient is directly proportional to how much that feature map is important for that prediction.\n","\n","To generate the CAM just multiply each activation features with the respective average gradients, the higher the gradient for that feature map the more important is that feature map for the class predicted, and that will be the most predominant region in the output CAM."],"metadata":{"id":"he-vyzsC59Ku"}},{"cell_type":"code","source":["class GradCAM:\n","    def __init__(self, model, target_layer):\n","        \"\"\"\n","        Args:\n","            model: trained model\n","            target_layer: the layer to compute gradients from (e.g., model.features[-1])\n","        \"\"\"\n","        self.model = model\n","        self.target_layer = target_layer\n","        self.gradients = None\n","        self.activations = None\n","\n","        # Register hooks\n","        self.target_layer.register_forward_hook(self.save_activation)\n","        self.target_layer.register_backward_hook(self.save_gradient)\n","\n","    def save_activation(self, module, input, output):\n","        self.activations = output.detach()\n","\n","    def save_gradient(self, module, grad_input, grad_output):\n","        self.gradients = grad_output[0].detach()\n","\n","    def generate_cam(self, input_image, target_class=None):\n","        \"\"\"\n","        Generate Class Activation Map\n","        Args:\n","            input_image: input tensor [1, C, H, W]\n","            target_class: target class index (if None, use predicted class)\n","        Returns:\n","            cam: class activation map\n","            pred_class: predicted class\n","        \"\"\"\n","        self.model.eval()\n","\n","        # Forward pass\n","        output = self.model(input_image)\n","\n","        if target_class is None:\n","            target_class = output.argmax(dim=1).item()\n","\n","        # Backward pass\n","        self.model.zero_grad()\n","        class_loss = output[0, target_class]\n","        class_loss.backward()\n","\n","        # Generate CAM\n","        gradients = self.gradients[0]  # [C, H, W]\n","        activations = self.activations[0]  # [C, H, W]\n","\n","        # Global average pooling of gradients\n","        weights = gradients.mean(dim=(1, 2))  # [C]\n","\n","        # Weighted combination of activation maps\n","        cam = torch.zeros(activations.shape[1:], dtype=torch.float32).to(activations.device) # Fix: Initialize cam on the same device as activations\n","        for i, w in enumerate(weights):\n","            cam += w * activations[i]\n","\n","        # ReLU and normalize\n","        cam = F.relu(cam)\n","        cam = cam - cam.min()\n","        if cam.max() != 0:\n","            cam = cam / cam.max()\n","\n","        return cam.cpu().numpy(), target_class\n","\n","def apply_colormap_on_image(org_img, activation_map, colormap=cv2.COLORMAP_JET, alpha=0.5):\n","    \"\"\"\n","    Apply colormap on image\n","    Args:\n","        org_img: original image (H, W, 3) in range [0, 1]\n","        activation_map: activation map (H, W) in range [0, 1]\n","        colormap: opencv colormap\n","        alpha: transparency factor\n","    Returns:\n","        superimposed image\n","    \"\"\"\n","    # Resize activation map to match image size\n","    activation_map = cv2.resize(activation_map, (org_img.shape[1], org_img.shape[0]))\n","\n","    # Convert to uint8\n","    heatmap = cv2.applyColorMap(np.uint8(255 * activation_map), colormap)\n","    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n","    heatmap = np.float32(heatmap) / 255\n","\n","    # Superimpose heatmap on image\n","    cam_img = alpha * heatmap + (1 - alpha) * org_img\n","    cam_img = cam_img / cam_img.max()\n","\n","    return cam_img"],"metadata":{"id":"wlilFAGt07yy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Grad-CAM on Test Samples"],"metadata":{"id":"riBbwchy1IZz"}},{"cell_type":"code","source":["# Initialize Grad-CAM with the last convolutional layer\n","grad_cam = GradCAM(model, target_layer=model.features[-1])\n","\n","# Select random test samples to visualize\n","num_samples = 8\n","random_indices = np.random.choice(len(test_ds), num_samples, replace=False)\n","\n","plt.figure(figsize=(20, 10))\n","\n","for idx, sample_idx in enumerate(random_indices):\n","    # Get image and label\n","    img_tensor, true_label = test_ds[sample_idx]\n","    img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n","\n","    # Generate CAM\n","    cam, pred_class = grad_cam.generate_cam(img_tensor_batch)\n","\n","    # ---- FIXED DENORMALIZATION ----\n","    img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)\n","\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std  = np.array([0.229, 0.224, 0.225])\n","\n","    # Undo normalization\n","    img_np = img_np * std + mean\n","\n","    # Convert 0–255 → 0–1 (since Normalize was applied on uint8 images)\n","    img_np = img_np / 255.0\n","\n","    img_np = np.clip(img_np, 0, 1)\n","    # --------------------------------\n","\n","    # Apply colormap\n","    cam_img = apply_colormap_on_image(img_np, cam, alpha=0.5)\n","\n","    # Plot original image\n","    plt.subplot(4, num_samples // 2, idx + 1)\n","    plt.imshow(img_np)\n","    plt.title(f'True: {class_names[true_label]}\\nPred: {class_names[pred_class]}',\n","              fontsize=10, color='green' if true_label == pred_class else 'red')\n","    plt.axis('off')\n","\n","    # Plot Grad-CAM overlay\n","    plt.subplot(4, num_samples // 2, idx + 1 + num_samples)\n","    plt.imshow(cam_img)\n","    plt.title('Grad-CAM Activation', fontsize=10)\n","    plt.axis('off')\n","\n","plt.suptitle('Grad-CAM Visualizations: Model Attention Regions', fontsize=16, y=1.00)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"2Uu_28iM0_CM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analyze results"],"metadata":{"id":"XAjn1iMF1Op0"}},{"cell_type":"code","source":["# Visualize Grad-CAM for correct and incorrect predictions\n","model.eval()\n","\n","correct_samples = []\n","incorrect_samples = []\n","\n","# Find examples of correct and incorrect predictions\n","with torch.no_grad():\n","    for i in range(len(test_ds)):\n","        img, label = test_ds[i]\n","        img_batch = img.unsqueeze(0).to(device)\n","        output = model(img_batch)\n","        pred = output.argmax(dim=1).item()\n","\n","        if pred == label and len(correct_samples) < 4:\n","            correct_samples.append(i)\n","        elif pred != label and len(incorrect_samples) < 4:\n","            incorrect_samples.append(i)\n","\n","        if len(correct_samples) == 4 and len(incorrect_samples) == 4:\n","            break\n","\n","# Visualize\n","fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n","\n","mean = np.array([0.485, 0.456, 0.406])\n","std  = np.array([0.229, 0.224, 0.225])\n","\n","# ---------- CORRECT PREDICTIONS ----------\n","for i, sample_idx in enumerate(correct_samples):\n","    img_tensor, true_label = test_ds[sample_idx]\n","    img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n","\n","    cam, pred_class = grad_cam.generate_cam(img_tensor_batch)\n","\n","    # ---- FIXED DENORMALIZATION ----\n","    img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)\n","    img_np = img_np * std + mean     # undo normalization\n","    img_np = img_np / 255.0          # convert 0–255 → 0–1\n","    img_np = np.clip(img_np, 0, 1)\n","    # --------------------------------\n","\n","    cam_img = apply_colormap_on_image(img_np, cam, alpha=0.5)\n","\n","    axes[i, 0].imshow(img_np)\n","    axes[i, 0].set_title(f'Correct: {class_names[true_label]}', color='green')\n","    axes[i, 0].axis('off')\n","\n","    axes[i, 1].imshow(cam_img)\n","    axes[i, 1].set_title('Grad-CAM', color='green')\n","    axes[i, 1].axis('off')\n","\n","# ---------- INCORRECT PREDICTIONS ----------\n","for i, sample_idx in enumerate(incorrect_samples):\n","    img_tensor, true_label = test_ds[sample_idx]\n","    img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n","\n","    cam, pred_class = grad_cam.generate_cam(img_tensor_batch)\n","\n","    # ---- FIXED DENORMALIZATION ----\n","    img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)\n","    img_np = img_np * std + mean     # undo normalization\n","    img_np = img_np / 255.0          # convert 0–255 → 0–1\n","    img_np = np.clip(img_np, 0, 1)\n","    # --------------------------------\n","\n","    cam_img = apply_colormap_on_image(img_np, cam, alpha=0.5)\n","\n","    axes[i, 2].imshow(img_np)\n","    axes[i, 2].set_title(\n","        f'True: {class_names[true_label]}\\nPred: {class_names[pred_class]}',\n","        color='red', fontsize=9\n","    )\n","    axes[i, 2].axis('off')\n","\n","    axes[i, 3].imshow(cam_img)\n","    axes[i, 3].set_title('Grad-CAM', color='red')\n","    axes[i, 3].axis('off')\n","\n","fig.text(0.25, 0.98, 'Correct Predictions', ha='center', fontsize=14, weight='bold')\n","fig.text(0.75, 0.98, 'Incorrect Predictions', ha='center', fontsize=14, weight='bold')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"Iqv7JdEz1Qrf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Per class visualization"],"metadata":{"id":"9DYBXgfa1jjx"}},{"cell_type":"code","source":["# Analyze attention patterns per class\n","plt.figure(figsize=(20, 12))\n","\n","for class_idx, class_name in enumerate(class_names):\n","    print(class_name)\n","\n","for class_idx, class_name in enumerate(class_names):\n","    # Find samples of this class\n","    class_samples = [i for i in range(len(test_ds)) if test_ds[i][1] == class_idx][:3]\n","\n","    for sample_num, sample_idx in enumerate(class_samples):\n","        img_tensor, true_label = test_ds[sample_idx]\n","        img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n","\n","        cam, pred_class = grad_cam.generate_cam(img_tensor_batch, target_class=class_idx)\n","\n","        # ---- FIXED DENORMALIZATION ----\n","        img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)\n","\n","        mean = np.array([0.485, 0.456, 0.406])\n","        std  = np.array([0.229, 0.224, 0.225])\n","\n","        img_np = img_np * std + mean      # undo normalization\n","        img_np = img_np / 255.0           # convert from 0–255 → 0–1\n","        img_np = np.clip(img_np, 0, 1)\n","        # --------------------------------\n","\n","        cam_img = apply_colormap_on_image(img_np, cam, alpha=0.5)\n","\n","        plt.subplot(len(class_names), 3, class_idx * 3 + sample_num + 1)\n","        plt.imshow(cam_img)\n","        if sample_num == 0:\n","            plt.ylabel(class_name, fontsize=12, weight='bold')\n","        plt.axis('off')\n","\n","plt.suptitle('Grad-CAM Activation Patterns Per Class', fontsize=16, y=0.995)\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"AsRFKBVb1jKy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Produce Meningioma image"],"metadata":{"id":"6Fks-8lFgQcC"}},{"cell_type":"code","source":["\n","# Initialize Grad-CAM with the last convolutional layer\n","grad_cam = GradCAM(model, target_layer=model.features[-1])\n","\n","# Find a meningioma sample (class_names should have 'meningioma')\n","meningioma_idx = np.where(class_names == 'pituitary')[0][0]\n","\n","# Find a test sample with meningioma\n","meningioma_samples = [i for i in range(len(test_ds)) if test_ds[i][1] == meningioma_idx]\n","sample_idx = meningioma_samples[0]  # Take the first meningioma sample\n","\n","# Get image and label\n","img_tensor, true_label = test_ds[sample_idx]\n","img_tensor_batch = img_tensor.unsqueeze(0).to(device)\n","\n","# Generate CAM\n","cam, pred_class = grad_cam.generate_cam(img_tensor_batch)\n","\n","# ---- FIXED DENORMALIZATION ----\n","img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)\n","\n","mean = np.array([0.485, 0.456, 0.406])\n","std  = np.array([0.229, 0.224, 0.225])\n","\n","# Undo normalization\n","img_np = img_np * std + mean\n","\n","# Your original images were 0–255 uint8 before normalization,\n","# so convert correctly to 0–1 for display:\n","img_np = img_np / 255.0\n","\n","img_np = np.clip(img_np, 0, 1)\n","# --------------------------------\n","\n","# Apply colormap\n","cam_img = apply_colormap_on_image(img_np, cam, alpha=0.5)\n","\n","# Plot side by side\n","plt.figure(figsize=(12, 5))\n","\n","# Original image\n","plt.subplot(1, 2, 1)\n","plt.imshow(img_np)\n","plt.title(f'Original MRI\\nTrue: {class_names[true_label]}\\nPred: {class_names[pred_class]}',\n","          fontsize=14, color='green' if true_label == pred_class else 'red')\n","plt.axis('off')\n","\n","# Grad-CAM overlay\n","plt.subplot(1, 2, 2)\n","plt.imshow(cam_img)\n","plt.title('Grad-CAM Activation Heatmap', fontsize=14)\n","plt.axis('off')\n","\n","plt.suptitle('pituitary: Original vs Grad-CAM', fontsize=16, weight='bold')\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"sWRdg34MgS_l"},"execution_count":null,"outputs":[]}]}